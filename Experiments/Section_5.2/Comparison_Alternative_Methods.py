# -*- coding: utf-8 -*-
"""
Created on Mon Jan 22 12:32:57 2024

@author: ThomasDM
"""
import numpy as np
import pandas as pd

num_instances = 25
profit_diff = np.zeros((num_instances+2, 5))
profit = np.zeros((num_instances,7))
j=0
range_=[0,1,2,3,4,5,6,8,9,10,11,13,14,15,16,18,19,20,21,23,24,25,26,28,29] #avoid counting default instance several times

for i in range_: 

    i = float(i+1)
    RH_1_12 = np.load('../../RH_Strategies/RH_1_12/score_history'+str(i)+'_1_12.npy')
    RH_1_12 = np.mean(RH_1_12)
    
    RH_4_12 = np.load('../../RH_Strategies/RH_4_12/score_history'+str(i)+'_4_12.npy')
    RH_4_12 = np.mean(RH_4_12)

    RH_36_36 = np.load('../../RH_Strategies/RH_36_36/score_history'+str(i)+'_36_36.npy')
    RH_36_36 = np.mean(RH_36_36)

    PPPO = np.load('../../TR_Approach/TRL_Evaluations/evaluations_PPPO_' + str(i) +'.npz')
    PPPO = PPPO['results']
    PPPO = PPPO[-1,:]
    PPPO = np.mean(PPPO)
    
    PPO_1 = np.load('../../PPO/PPO_0.0001/evaluations_PPO_' + str(i) +'.npz')
    PPO_1 = PPO_1['results']
    PPO_1 = np.mean(PPO_1, axis=1)
    PPO_1 = np.max(PPO_1)

    PPO_3 = np.load('../../PPO/PPO_0.0003/evaluations_PPO_' + str(i) +'.npz')
    PPO_3 = PPO_3['results']
    PPO_3 = np.mean(PPO_3, axis=1)
    PPO_3 = np.max(PPO_3)
    
    PPO_default = np.load('../../PPO/PPO_default_instance_0.0001/evaluations_PPO_' + str(i) +'.npz')
    PPO_default = PPO_default['results']
    PPO_default = np.mean(PPO_default, axis=1)
    PPO_default = np.max(PPO_default)

    perc_RH_1_12=(PPPO-RH_1_12)/RH_1_12
    perc_RH_4_12=(PPPO-RH_4_12)/RH_4_12
    perc_RH_36_36=(PPPO-RH_36_36)/RH_36_36

    perc_PPO_1=(PPPO-PPO_1)/PPO_1
    perc_PPO_default=(PPPO-PPO_default)/PPO_default
    
    perc_PPO_3=(PPPO-PPO_3)/PPO_3

    profit_diff[j, 0] = perc_RH_36_36
    profit_diff[j, 1] = perc_RH_4_12
    profit_diff[j, 2] = perc_RH_1_12
    profit_diff[j, 3] = perc_PPO_1
    profit_diff[j, 4] = perc_PPO_3
    
    profit[j, 0] = RH_36_36
    profit[j, 1] = RH_4_12
    profit[j, 2] = RH_1_12
    profit[j, 3] = PPO_1
    profit[j, 4] = PPO_3
    profit[j, 5] = PPPO
    profit[j, 6] = PPO_default

    j+=1
    
profit_diff[num_instances, 0] = np.mean(profit_diff[0:num_instances, 0])
profit_diff[num_instances, 1] = np.mean(profit_diff[0:num_instances, 1])
profit_diff[num_instances, 2] = np.mean(profit_diff[0:num_instances, 2])
profit_diff[num_instances, 3] = np.mean(profit_diff[0:num_instances, 3])
profit_diff[num_instances, 4] = np.mean(profit_diff[0:num_instances, 4])

profit_diff[num_instances+1, 0] = np.median(profit_diff[0:num_instances, 0])
profit_diff[num_instances+1, 1] = np.median(profit_diff[0:num_instances, 1])
profit_diff[num_instances+1, 2] = np.median(profit_diff[0:num_instances, 2])
profit_diff[num_instances+1, 3] = np.median(profit_diff[0:num_instances, 3])
profit_diff[num_instances+1, 4] = np.median(profit_diff[0:num_instances, 4])

DF = pd.DataFrame(profit_diff) 

"After visualizing the learning curves for each instance, we noitced that our criterion stop was not strict enough during learning (in particular for PPO without pretraining)."
"To avoid rerunning the whole set of experiments, we decided to reselect the number of evaluations after which learning is stopped based on a stricter stopping criterion: At least 50 evaluation epochs, and then, stop if 10 evaluation epochs without improvement. If 10 evaluations without improvement but have never reached at least 85% of the profit generated by our approach, then continue."
"The running time was adjusted accordingly in an Excel sheet."
"Example: given an initial running time of PPO (lr = 0.0001) = 15511 minutes for a number of evaluations epoch = 100, we can extrapolate that after 70 evaluation epochs (the number of evaluation epochs after which the algo should have stopped according to our new stopping criterion), the running time should have been approximately=15511*70/100=10857 "

num_iterations = np.zeros((num_instances,4))
l = 0
for i in range_: 
    i = float(i+1)

    PPO_1 = np.load('../../PPO/PPO_0.0001/evaluations_PPO_' + str(i) +'.npz')
    PPO_1 = PPO_1['results']
    PPO_1 = np.mean(PPO_1, axis=1)

    PPO_2 = np.load('../../PPO/PPO_default_instance_0.0001/evaluations_PPO_' + str(i) +'.npz')
    PPO_2 = PPO_2['results']
    PPO_2 = np.mean(PPO_2, axis=1)
    
    PPO_3 = np.load('../../PPO/PPO_0.0003/evaluations_PPO_' + str(i) +'.npz')
    PPO_3 = PPO_3['results']
    PPO_3 = np.mean(PPO_3, axis=1)
    
    PPPO = np.load('../../TR_Approach/TRL_Evaluations/evaluations_PPPO_' + str(i) +'.npz')
    PPPO = PPPO['results']
    PPPO = np.mean(PPPO, axis=1)

    max_p = PPPO[0]
    k=0
    
    for j in range(1, len(PPPO)):
        PPPO_eval = PPPO[j]
        perc = (PPPO_eval-max_p)/max_p

        if PPPO_eval >= max_p:
            improv = True
            max_p= PPPO_eval 
            k = 0
        else:
            improv = False
            k += 1
        
        if j > 50:
            if k == 10:
                if max_p > 0:
                    num_iterations[l, 3] = j +1
                    break
               
            if j ==len(PPPO)-1:
                num_iterations[l, 3] = j +1
    
    max_1 = PPO_1[0]
    k=0
    
    for j in range(1, len(PPO_1)):
        PPO_eval = PPO_1[j]
        perc = (PPO_eval-max_1)/max_1

        if PPO_eval >= max_1:
            improv = True
            max_1 = PPO_eval 
            k = 0
        else:
            improv = False
            k += 1
        
        if j > 50:
            if k == 10:
                if max_1 > 0.85 * max_p :
                    num_iterations[l, 0] = j+1
                    break
            
            if j ==len(PPO_1)-1:
                num_iterations[l, 0] = j +1
            
    max_2 = PPO_2[0]
    k=0
    for j in range(1, len(PPO_2)):
        PPO_eval = PPO_2[j]
        perc = (PPO_eval-max_2)/max_2

        if PPO_eval >= max_2:
            improv = True
            max_2 = PPO_eval 
            k = 0
        else:
            improv = False
            k += 1
            
        if j > 50:
            if k == 10:
                if max_2 > 0.85* max_p:
                    num_iterations[l, 1] = j +1
                    break
                
            if j ==len(PPO_2)-1:
                num_iterations[l, 1] = j +1
            
            
    max_3 = PPO_3[0]
    k=0
    for j in range(1, len(PPO_3)):
        PPO_eval = PPO_3[j]
        perc = (PPO_eval-max_3)/max_3

        if PPO_eval >= max_3:
            improv = True
            max_3 = PPO_eval 
            k = 0
        else:
            improv = False
            k += 1
            
        if j > 50:
            if k == 10:
                if max_3 >  0.85* max_p:
                    num_iterations[l, 2] = j +1
                    break
                
            if j ==len(PPO_3)-1:
                num_iterations[l, 2] = j +1
                
    l+=1
            
    
        
        
        

